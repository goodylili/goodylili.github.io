<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    

    
        <title>
             Using LLamaIndex to add Personal Data to LLMs
            
        </title>

        
            <meta property="og:title" content="Using LLamaIndex to add Personal Data to LLMs" />
        
    

    
        
            <meta property="og:description" content="Learn how to build and customize LLMs for your projects with LlamaIndex" />
        
    

    
        
            <meta name="description" content="Learn how to build and customize LLMs for your projects with LlamaIndex" />
        
    

    
         <link rel="icon" type="image/png" href=&#x2F;icon&#x2F;favicon.png />
    

    

    

    
    
        <script src=//goodylili.github.io/js/feather.min.js></script>
    


    
        <link href=//goodylili.github.io/css/fonts.css rel="stylesheet" />
    

    <link rel="stylesheet" type="text/css" media="screen" href=//goodylili.github.io/css/main.css />

    
        <link
            rel="stylesheet"
            id="darkModeStyle"
            type="text/css"
            href=//goodylili.github.io/css/dark.css
            
            
                disabled
            
        />
    


    


</head>


<body>
    <div class="content">
        <header>
    <div class="main" id="main_title">
        <a href=&#x2F;&#x2F;goodylili.github.io&#x2F;>Ukeje C Goodness</a>
    </div>

    <nav>
        
            <a href=&#x2F;>Home</a>
        
            <a href=&#x2F;posts>All posts</a>
        
            <a href=&#x2F;about>About</a>
        
            <a href=&#x2F;tags>Tags</a>
        

        
            |

            
                <a href=&#x2F;>en</a>
            
        

        
            | <a id="dark-mode-toggle" onclick="toggleTheme()" href=""></a>
            <script src=//goodylili.github.io/js/themetoggle.js></script>
        

    <! -- GoadCounter analytics script -->
        <script data-goatcounter="https://ghostmac.goatcounter.com/count"
                async src="//gc.zgo.at/count.js"></script>
    </nav>
</header>


        
    
<main>
    <article>
        <div class="title">
            <h1 class="title">Using LLamaIndex to add Personal Data to LLMs</h1>
            <div class="meta">
                
                on  2024-07-23

                
            </div>
        </div>

        

        <section class="body">
            <hr />
<p><em><a href="//goodylili.github.io/posts/using-llamaindex-to-add-personal-data-to-llm/logrocket.com">LogRocket</a> made this piece possible. They provide AI-first session replay and analytics that shows
you what's wrong.</em></p>
<h1 id="using-llamaindex-to-add-personal-data-to-llm">Using LlamaIndex to add Personal data to LLM</h1>
<p>Retrieval-augmented generation (RAG) integrates retrieval mechanisms with large language models (LLMs) to generate contextually relevant text. RAGs divide documents into chunks before retrieving relevant chunk-based queries and augment input prompts with chunks as context to answer queries.</p>
<p>There are many RAG software options in the market, but the most popular is LlamaIndex due to how easy it is to set up and use. Let’s see how to use LlamaIndex to add personal data to an LLM.</p>
<h2 id="what-is-llamaindex">What is LlamaIndex?</h2>
<p><a href="https://www.llamaindex.ai/">LlamaIndex</a> is a data framework that enhances the capabilities of LLMs through context augmentation. This RAG framework is useful for building apps that require LLMs to operate on private or domain-specific data not built into an LLM.</p>
<p>LlamaIndex provides tools for ingesting, processing, and implementing complex query workflows that combine data access with LLM prompting.</p>
<p>You can build many different projects with LlamaIndex, from QA chatbots to document understanding and extraction to autonomous agents that can perform research and execute actions. It’s available to use in Python and TypeScript to help you build AI-composed apps.</p>
<H3>Drawbacks of using LLamaIndex</H3>
<p>LlamaIndex is really great software, but there are some drawbacks you may encounter when you try using it to add personal data to LLMs. Two of the most important considerations to keep in mind are:</p>
<ul>
<li><strong>Performance and scalability</strong> : LlamaIndex can be resource-intensive depending on the scale of data that you’re indexing. This may affect your app's performance if you’re running with limited hardware</li>
<li><strong>Integration challenge</strong>s : Integrating LlamaIndex with existing systems might increase the complexity of your existing systems, especially if you haven’t designed it with integrations in view.</li>
</ul>
<p>Nonetheless, LlamaIndex is the most popular RAG tool with the most community support, so you should easily find help with navigating these issues.</p>
<H3>Use cases and applications of RAGs like LlamaIndex</H3>
<p>You can use LlamaIndex when building applications that use AI for chatbots, data retrieval, multimodal interactions, and many other use cases. Since you can add custom data, you can also use LlamaIndex to improve your product offerings.</p>
<p>Here are some integrations where it makes sense to use LlamaIndex and other RAG tools:</p>
<ul>
<li><strong>Documentation and FAQ support</strong>: You can use LlamaIndex to create self-service documentation and FAQ systems to ease comprehension and UX. You can also train the LLM on code snippets and internal documentation to improve efficiency and consistency</li>
<li><strong>Self-service</strong>: Adding RAGs to your apps can help customers solve their issues easily without queuing to get responses from a customer service agent</li>
<li><strong>Internal code generation</strong>: You can train local LLMs to help improve productivity, avoid leaking sensitive data, and prevent the risks of using public LLMs</li>
</ul>
<p>These are just a few of the many possibilities you can achieve using RAGs like LlamaIndex in development and production.</p>
<h2 id="adding-your-data-to-llms-with-llamaindex">Adding your data to LLMs with LlamaIndex</h2>
<p>RAGs, LLMs, and AI can be daunting subjects for developers, but LlamaIndex is quite straightforward. First, execute this command to create a new virtual environment for this project:</p>
<pre data-lang="python" style="background-color:#fafafa;color:#383a42;" class="language-python "><code class="language-python" data-lang="python"><span>    python </span><span style="color:#a626a4;">-</span><span>m venv llama 
</span></code></pre>
<p>Activate the virtual environment with this command:</p>
<pre data-lang="python" style="background-color:#fafafa;color:#383a42;" class="language-python "><code class="language-python" data-lang="python"><span>    source llama</span><span style="color:#a626a4;">/</span><span style="color:#0184bc;">bin</span><span style="color:#a626a4;">/</span><span>activate
</span></code></pre>
<p>Now, install all the dependencies you need to follow through with a simple <code>pip``3</code> <code>install</code> command:</p>
<pre data-lang="python" style="background-color:#fafafa;color:#383a42;" class="language-python "><code class="language-python" data-lang="python"><span>    pip3 install llama</span><span style="color:#a626a4;">-</span><span>index python</span><span style="color:#a626a4;">-</span><span>dotenv pandas
</span></code></pre>
<p>Let’s quickly review the dependencies we just installed:</p>
<ul>
<li><code>llama-index</code> is the library you’ll use to add your data to LLMs</li>
<li><code>python-dotenv</code> is a package that helps with managing environment variables</li>
<li><code>pandas</code> is a popular and powerful library for data analytics and manipulation</li>
</ul>
<p>You can use Pandas to feed the LLM with structured data like CSV files and then query the LLM for feedback based on the data.</p>
<p>In a real-world app, you’ll likely use customer data or data related to your project. You’ll also likely have a data preparation or manipulation process in place. However, for the purposes of this tutorial, we’ll use <a href="https://www.kaggle.com/datasets/sudalairajkumar/cryptocurrencypricehistory">this ready-made data on</a> <a href="https://www.kaggle.com/datasets/sudalairajkumar/cryptocurrencypricehistory">c</a><a href="https://www.kaggle.com/datasets/sudalairajkumar/cryptocurrencypricehistory">ryptocurrency prices from Kaggle</a> to train the LLM.</p>
<p>Download the dataset from Kaggle, extract the files and add them to a folder in the root directory of your project.</p>
<p>Finally, you’ll need a model to use LlamaIndex. LlamaIndex supports multiple models, and you can use local models as well. We’ll use OpenAI, so <a href="http://platform.openai.com/api-keys">create a project</a> and get an API key that you can use to request any of the OpenAI models. Add the environment variable to a <code>.env</code> file with the key <code>OPENAI_API_KEY</code> and you’re good to roll.</p>
<p>Now, let’s go over how you can build a knowledge base with LlamaIndex by adding custom data to the LLM.</p>
<h2 id="building-your-knowledge-base-with-llamaindex">Building your knowledge base with LlamaIndex</h2>
<p>Open the Python file in your project and add these imports:</p>
<pre data-lang="python" style="background-color:#fafafa;color:#383a42;" class="language-python "><code class="language-python" data-lang="python"><span>    </span><span style="color:#a626a4;">from </span><span>llama_index.core </span><span style="color:#a626a4;">import </span><span>TreeIndex, SimpleDirectoryReader
</span><span>    
</span><span>    </span><span style="color:#a626a4;">import </span><span>os
</span></code></pre>
<p>You’ll use the <code>os</code> module to load environment variables, but you can do much more, including interacting with the operating system and IO operations on files.</p>
<p>The <code>llama_index.core</code> module contains core functions for working with LlamaIndex. The <code>TreeIndex</code> module helps create and manage indexes in a tree structure, while the <code>SimpleDirectoryReader</code> module reads data from directories.</p>
<p>Next, you must load the OpenAI API key from the environment variables file. Here’s how you can do that with the <code>dotenv</code> module:</p>
<pre data-lang="python" style="background-color:#fafafa;color:#383a42;" class="language-python "><code class="language-python" data-lang="python"><span>    </span><span style="color:#a626a4;">from </span><span>dotenv </span><span style="color:#a626a4;">import </span><span>load_dotenv  
</span><span>    
</span><span>    </span><span style="color:#a0a1a7;"># Load environment variables from .env file
</span><span>    </span><span style="color:#e45649;">load_dotenv</span><span>() 
</span><span>    
</span><span>    </span><span style="color:#a0a1a7;"># Access the environment variable
</span><span>    api_key </span><span style="color:#a626a4;">= </span><span>os.</span><span style="color:#e45649;">getenv</span><span>(</span><span style="color:#50a14f;">&quot;OPENAI_API_KEY&quot;</span><span>)
</span><span>    
</span><span>    </span><span style="color:#a0a1a7;"># Ensure the API key is loaded
</span><span>    </span><span style="color:#a626a4;">if not </span><span>api_key:
</span><span>        </span><span style="color:#a626a4;">raise </span><span style="color:#e45649;">ValueError</span><span>(</span><span style="color:#50a14f;">&quot;OPENAI_API_KEY is not set in the .env file&quot;</span><span>)
</span><span>    
</span><span>    os.environ[</span><span style="color:#50a14f;">&quot;OPENAI_API_KEY&quot;</span><span>] </span><span style="color:#a626a4;">= </span><span>api_key
</span></code></pre>
<p>You can use the <code>SimpleDirectoryReader</code> to easily read files in a directory — in this case, the local <code>Crypto</code> directory I have, which contains a <code>bitcoin</code> PDF file — and load the data with the <code>load_data</code> function:</p>
<pre data-lang="python" style="background-color:#fafafa;color:#383a42;" class="language-python "><code class="language-python" data-lang="python"><span>    bitcoin </span><span style="color:#a626a4;">= </span><span style="color:#e45649;">SimpleDirectoryReader</span><span>(</span><span style="color:#50a14f;">&quot;crypto&quot;</span><span>).</span><span style="color:#e45649;">load_data</span><span>()
</span><span>    new_index </span><span style="color:#a626a4;">= </span><span>TreeIndex.</span><span style="color:#e45649;">from_documents</span><span>(bitcoin)
</span></code></pre>
<p>The <code>from_documents</code> function of the <code>TreeIndex</code> class creates an index from the documents you specify.</p>
<p>Finally, you can instantiate a query engine to query the LLM with the <code>as_query_engine</code> function and make a query with the <code>query</code> function:</p>
<pre data-lang="python" style="background-color:#fafafa;color:#383a42;" class="language-python "><code class="language-python" data-lang="python"><span>    query_engine </span><span style="color:#a626a4;">= </span><span>new_index.</span><span style="color:#e45649;">as_query_engine</span><span>()
</span><span>    response </span><span style="color:#a626a4;">= </span><span>query_engine.</span><span style="color:#e45649;">query</span><span>(</span><span style="color:#50a14f;">&quot;What is Bitcoin&quot;</span><span>)
</span><span>    
</span><span>    </span><span style="color:#0184bc;">print</span><span>(response)
</span></code></pre>
<p>The <code>response</code> variable should hold the output of the query.</p>
<p>Here’s the output from querying the LLM:</p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_1A4DBF8198A1E7F8E044895A713D54DFEC71A8234F32ECE7E7D8A54185E0EA59_1721655328005_Untitled+20.png" alt="" /></p>
<p>The method above is a question-and-answer method where you query the LLM for answers. As discussed earlier, you can also build chatbots and many other AI applications with LlamaIndex.</p>
<p>Here’s how you can build a chatbot with LlamaIndex:</p>
<pre data-lang="python" style="background-color:#fafafa;color:#383a42;" class="language-python "><code class="language-python" data-lang="python"><span>    </span><span style="color:#a626a4;">from </span><span>llama_index.core </span><span style="color:#a626a4;">import </span><span>TreeIndex, SimpleDirectoryReader
</span><span>    
</span><span>    </span><span style="color:#a626a4;">import </span><span>os
</span><span>    
</span><span>    </span><span style="color:#a626a4;">from </span><span>dotenv </span><span style="color:#a626a4;">import </span><span>load_dotenv  
</span><span>    
</span><span>    </span><span style="color:#a0a1a7;"># Load environment variables from .env file
</span><span>    </span><span style="color:#e45649;">load_dotenv</span><span>() 
</span><span>    
</span><span>    </span><span style="color:#a0a1a7;"># Access the environment variable
</span><span>    api_key </span><span style="color:#a626a4;">= </span><span>os.</span><span style="color:#e45649;">getenv</span><span>(</span><span style="color:#50a14f;">&quot;OPENAI_API_KEY&quot;</span><span>)
</span><span>    
</span><span>    </span><span style="color:#a0a1a7;"># Ensure the API key is loaded
</span><span>    </span><span style="color:#a626a4;">if not </span><span>api_key:
</span><span>        </span><span style="color:#a626a4;">raise </span><span style="color:#e45649;">ValueError</span><span>(</span><span style="color:#50a14f;">&quot;OPENAI_API_KEY is not set in the .env file&quot;</span><span>)
</span><span>    
</span><span>    os.environ[</span><span style="color:#50a14f;">&quot;OPENAI_API_KEY&quot;</span><span>] </span><span style="color:#a626a4;">= </span><span>api_key
</span><span>    
</span><span>    </span><span style="color:#a0a1a7;"># read the directory
</span><span>    bitcoin </span><span style="color:#a626a4;">= </span><span style="color:#e45649;">SimpleDirectoryReader</span><span>(</span><span style="color:#50a14f;">&quot;crypto&quot;</span><span>).</span><span style="color:#e45649;">load_data</span><span>()
</span><span>    
</span><span>    </span><span style="color:#a0a1a7;"># index the directory tree
</span><span>    new_index </span><span style="color:#a626a4;">= </span><span>TreeIndex.</span><span style="color:#e45649;">from_documents</span><span>(bitcoin)
</span><span>    
</span><span>    </span><span style="color:#a0a1a7;"># instantiate a query engine instance
</span><span>    query_engine </span><span style="color:#a626a4;">= </span><span>new_index.</span><span style="color:#e45649;">as_query_engine</span><span>()
</span><span>    
</span><span>    response </span><span style="color:#a626a4;">= </span><span>query_engine.</span><span style="color:#e45649;">chat</span><span>(</span><span style="color:#50a14f;">&quot;Who Wrote this document?&quot;</span><span>)
</span><span>    </span><span style="color:#0184bc;">print</span><span>(response)
</span></code></pre>
<p>In this case, you’re using the <code>chat</code> function with the <code>query_engine</code> instance for chat functionality in contrast to the <code>query</code> function.</p>
<p>Here’s the output in this case:</p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_1A4DBF8198A1E7F8E044895A713D54DFEC71A8234F32ECE7E7D8A54185E0EA59_1721655327929_Untitled+19.png" alt="" /></p>
<p>Head over to <a href="https://docs.llamaindex.ai/">the LlamaIndex documentation</a> to learn more about what you can build with LlamaIndex and the tools and models you can use with the LLM.</p>
<h2 id="llamaindex-alternatives">LlamaIndex alternatives</h2>
<p>There are other RAG tools you can explore if LlamaIndex doesn't suit your needs. Let’s take a look at two popular alternatives: LangChain and Vellum.</p>
<H3>LangChain</H3>
<p>LangChain is an open source framework for building apps with LLMs. It supports various LLM tasks beyond RAGs and offers tools for prompt engineering, AI workflows, customization, scaling, lifecycle management, and more:</p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_1A4DBF8198A1E7F8E044895A713D54DFEC71A8234F32ECE7E7D8A54185E0EA59_1721655328612_Untitled+18.png" alt="" /></p>
<p>You can use LangChain to build applications and collaborate through LangSmith, and it’s flexible for handling prompts. LangChain offers more pre-built components than LLamaIndex, although customization and scaling might be challenging since the components are over-engineered.</p>
<H3>Vellum</H3>
<p>Vellum is a dedicated LLM product developer tool that offers scalability and advanced customizations for devs and product managers to build AI-ready apps:</p>
<p><img src="https://paper-attachments.dropboxusercontent.com/s_1A4DBF8198A1E7F8E044895A713D54DFEC71A8234F32ECE7E7D8A54185E0EA59_1721655327762_Untitled+17.png" alt="" /></p>
<p>You can use Vellum if you want a LlamaIndex alternative that’s easy to customize and adopt. It addresses some of the complexities and limitations you may face with LlamaIndex and LangChain.</p>
<p>Take a look at this table comparing LlamaIndex, LangChain, and Vellum:</p>
<table><thead><tr><th>Feature</th><th>LlamaIndex</th><th>LangChain</th><th>Vellum</th></tr></thead><tbody>
<tr><td>RAG applications</td><td>Seamless data indexing and retrieval</td><td>Supports RAG architectures but complex to scale</td><td>Focuses on production-ready AI apps with ease of use</td></tr>
<tr><td>AI workflows</td><td>Primarily for RAG architectures</td><td>More out-of-the-box components for diverse workflows</td><td>End-to-end development platform for AI apps</td></tr>
<tr><td>Prompt engineering</td><td>Basic prompt templates</td><td>LangSmith for prompt organization and versioning</td><td>Advanced prompt customization and comparison</td></tr>
<tr><td>Evaluation</td><td>Component-based evaluation for RAG</td><td>LangSmith evaluator suite for general LLM tasks</td><td>Comprehensive evaluation at scale</td></tr>
<tr><td>Lifecycle management</td><td>Integrations with observability tools</td><td>LangSmith for detailed debugging and monitoring</td><td>Full lifecycle management with version control</td></tr>
<tr><td>Scalability</td><td>Challenges with customization and complexity</td><td>More pre-built components but complex to manage</td><td>Built for scalability and advanced customizations</td></tr>
<tr><td>Community &amp; improvements</td><td>Active community, proprietary core</td><td>Active community, open-source contributions</td><td>Continuous improvement with cross-functional collaboration</td></tr>
</tbody></table>
<h2 id="conclusion">Conclusion</h2>
<p>In this tutorial, we learned about LlamaIndex and how you can use this RAG tool to add personal data to LLMs for the various use cases you may have. We explored a practical example using a sample dataset from Kaggle and then set up a simple chatbot with LlamaIndex.</p>
<p>Beyond the demos we saw in this article, you can do so much with customized LLMs in development and production. LlamaIndex and other tools make advanced application features possible, from question-and-answer features to chatbots and more. Happy coding!</p>

        </section>

        
            <div class="post-tags">
                <nav class="nav tags">
                    <ul class="tags">
                        
                            <li><a href=//goodylili.github.io/tags/rust/>Rust</a></li>
                        
                            <li><a href=//goodylili.github.io/tags/development/>development</a></li>
                        
                            <li><a href=//goodylili.github.io/tags/tools/>Tools</a></li>
                        
                            <li><a href=//goodylili.github.io/tags/technical/>Technical</a></li>
                        
                            <li><a href=//goodylili.github.io/tags/data/>data</a></li>
                        
                    </ul>
                </nav>
            </div>
        

    </article>
</main>



        <footer>
    <div style="display:flex">
        
        <a class="soc" href=https:&#x2F;&#x2F;github.com&#x2F;goodylili&#x2F; title=GitHub>
            <i data-feather=GitHub></i>
        </a>
        
        <a class="soc" href=https:&#x2F;&#x2F;twitter.com&#x2F;goodylili title=Twitter>
            <i data-feather=twitter></i>
        </a>
        
        <a class="soc" href=ukejegoodness599@gmail.com title=mail>
            <i data-feather=mail></i>
        </a>
        
    </div>
    <div class="footer-info">
        2025 © goodylili 👻 |
        <a href="https://docs.google.com/document/d/1eRrR-blUADxdC7brWlGTye8yd_NQDl2Eac8TB1l5N1o/edit?usp=sharing">My CV</a>
    </div>
</footer>


<script>
    feather.replace();
</script>

    </div>
</body>

</html>
